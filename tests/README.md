# Legal AI System - Test Suite

Comprehensive test suite achieving 95% code coverage for the Legal AI System.

## Overview

This test suite provides comprehensive coverage of all system components including:
- **Backend API** (Python FastAPI)
- **Frontend** (Next.js/React)
- **Database Operations** (PostgreSQL)
- **Integration Workflows**
- **Security & Performance**

## ğŸ¯ Coverage Targets

- **Overall Coverage**: 95% minimum
- **Backend Coverage**: 95% lines/branches
- **Frontend Coverage**: 95% lines/functions/statements
- **Integration Coverage**: 90% minimum

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Global pytest configuration
â”œâ”€â”€ pytest.ini                    # Pytest settings
â”œâ”€â”€ fixtures/                     # Test data and fixtures
â”‚   â”œâ”€â”€ sample_data.py            # Sample data generators
â”‚   â””â”€â”€ database_fixtures.py      # Database setup/teardown
â”œâ”€â”€ unit/                         # Unit tests
â”‚   â”œâ”€â”€ core/                     # Core system tests
â”‚   â”‚   â”œâ”€â”€ test_config.py        # Configuration tests
â”‚   â”‚   â””â”€â”€ test_app.py           # Application factory tests
â”‚   â””â”€â”€ shared/                   # Shared module tests
â”‚       â””â”€â”€ security/             # Security module tests
â”‚           â””â”€â”€ test_authentication.py
â”œâ”€â”€ integration/                  # Integration tests
â”‚   â”œâ”€â”€ api/                      # API integration tests
â”‚   â”‚   â””â”€â”€ test_health_endpoints.py
â”‚   â””â”€â”€ database/                 # Database integration tests
â”‚       â””â”€â”€ test_database_operations.py
â””â”€â”€ performance/                  # Performance benchmarks
    â””â”€â”€ test_load_scenarios.py

frontend/tests/
â”œâ”€â”€ jest.config.js               # Jest configuration
â”œâ”€â”€ jest.setup.js               # Test setup and mocks
â”œâ”€â”€ components/                 # Component unit tests
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â””â”€â”€ Layout.test.tsx     # Layout component tests
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ LiveMetrics.test.tsx # Dashboard tests
â”œâ”€â”€ integration/                # Frontend integration tests
â”‚   â””â”€â”€ document-workflow.test.tsx
â”œâ”€â”€ mocks/                      # Mock service handlers
â”‚   â”œâ”€â”€ handlers.js             # MSW request handlers
â”‚   â””â”€â”€ server.js              # Mock server setup
â””â”€â”€ utils/                      # Test utilities
    â””â”€â”€ test-utils.tsx         # Custom render functions
```

## ğŸš€ Running Tests

### Quick Start

```bash
# Run all tests with coverage
python scripts/run_tests.py

# Run only backend tests
python scripts/run_tests.py --type backend

# Run only frontend tests
python scripts/run_tests.py --type frontend

# Run specific test file
python scripts/run_tests.py --path tests/unit/core/test_config.py
```

### Backend Tests

```bash
# Unit tests only
pytest tests/unit/ --cov=src --cov-report=html

# Integration tests
pytest tests/integration/ --cov=src --cov-append

# Database tests
pytest tests/integration/database/ --cov=src --cov-append

# API tests
pytest tests/integration/api/ --cov=src --cov-append
```

### Frontend Tests

```bash
cd frontend/

# All tests with coverage
npm test -- --coverage --watchAll=false

# Component tests only
npm run test:components

# Integration tests only
npm run test:integration

# Watch mode for development
npm test
```

### Performance Tests

```bash
# Run performance benchmarks
pytest tests/performance/ --benchmark-only
```

## ğŸ“Š Coverage Reporting

### View Coverage Reports

```bash
# Generate coverage reports
python scripts/run_tests.py

# View HTML reports
# Backend: open coverage/backend-html/index.html
# Frontend: open frontend/coverage/lcov-report/index.html
```

### Coverage Configuration

**Backend** (`pyproject.toml`):
```toml
[tool.coverage.run]
source = ["src", "backend/app/src"]
branch = true
omit = ["*/tests/*", "*/__init__.py"]

[tool.coverage.report]
exclude_lines = ["pragma: no cover", "def __repr__"]
show_missing = true
skip_covered = false
precision = 2
fail_under = 95
```

**Frontend** (`frontend/tests/jest.config.js`):
```javascript
coverageThreshold: {
  global: {
    branches: 95,
    functions: 95,
    lines: 95,
    statements: 95
  }
}
```

## ğŸ—ï¸ Test Infrastructure

### Pytest Configuration

- **Async Support**: Full `pytest-asyncio` integration
- **Database Fixtures**: In-memory SQLite for fast tests
- **Mocking**: Comprehensive mocking for external services
- **Parallelization**: Support for `pytest-xdist`
- **Markers**: Custom markers for test categorization

### Jest Configuration

- **Next.js Integration**: Optimized for Next.js 14
- **MSW Mocking**: Mock Service Worker for API mocking
- **React Testing Library**: Component testing utilities
- **Custom Matchers**: Legal-specific test matchers

### CI/CD Integration

GitHub Actions workflow (`.github/workflows/tests.yml`):
- âœ… **Automated Testing**: All tests run on PR/push
- ğŸ“Š **Coverage Reports**: Automatic coverage reporting
- ğŸ”’ **Security Scanning**: CodeQL and Trivy scans
- ğŸš€ **Performance Tests**: Scheduled performance benchmarks

## ğŸ¨ Test Categories

### Unit Tests
- **Models**: Database model validation
- **Services**: Business logic testing  
- **Components**: React component testing
- **Utilities**: Helper function testing

### Integration Tests
- **API Endpoints**: Full request/response testing
- **Database Operations**: CRUD and transaction testing
- **Workflow Tests**: End-to-end user workflows
- **External Services**: Third-party API integration

### Performance Tests
- **Load Testing**: High-volume request handling
- **Memory Usage**: Memory leak detection
- **Response Times**: Latency benchmarking
- **Concurrency**: Concurrent user simulation

## ğŸ”§ Development Workflow

### Writing Tests

1. **Test Naming**: Use descriptive test names
   ```python
   def test_user_authentication_with_valid_credentials():
   def test_document_upload_handles_large_files():
   ```

2. **Arrange-Act-Assert**: Follow AAA pattern
   ```python
   def test_create_user():
       # Arrange
       user_data = {"email": "test@example.com"}
       
       # Act
       user = create_user(user_data)
       
       # Assert
       assert user.email == "test@example.com"
   ```

3. **Use Fixtures**: Leverage pytest fixtures
   ```python
   def test_user_creation(db_session, sample_user):
       # Test using pre-configured fixtures
   ```

### Mock External Services

```python
@patch('external_api.client.post')
def test_api_integration(mock_post):
    mock_post.return_value.json.return_value = {"status": "success"}
    # Test your code
```

### Frontend Component Testing

```typescript
import { render, screen } from '@testing-library/react'
import { Layout } from '@/components/layout/Layout'

test('renders navigation menu', () => {
  render(<Layout><div>Content</div></Layout>)
  expect(screen.getByRole('navigation')).toBeInTheDocument()
})
```

## ğŸ“ˆ Monitoring & Metrics

### Coverage Metrics
- **Line Coverage**: Code line execution
- **Branch Coverage**: Conditional branch testing  
- **Function Coverage**: Function call testing
- **Statement Coverage**: Statement execution

### Test Metrics
- **Test Count**: Total number of tests
- **Pass Rate**: Percentage of passing tests
- **Execution Time**: Test suite performance
- **Flaky Tests**: Inconsistent test identification

### Quality Gates

Before merging code:
- âœ… All tests must pass
- âœ… 95% coverage requirement met
- âœ… No security vulnerabilities
- âœ… Performance benchmarks passed

## ğŸ› Debugging Tests

### Common Issues

1. **Async Test Failures**
   ```python
   # Use pytest.mark.asyncio
   @pytest.mark.asyncio
   async def test_async_function():
       result = await async_function()
       assert result is not None
   ```

2. **Database Connection Issues**
   ```python
   # Use database fixtures
   async def test_with_db(async_db_session):
       # Test database operations
   ```

3. **Frontend Component Issues**
   ```typescript
   // Use proper async/await for user events
   const user = userEvent.setup()
   await user.click(button)
   ```

### Debug Commands

```bash
# Verbose output
pytest -v tests/unit/core/

# Stop on first failure
pytest -x tests/

# Debug specific test
pytest --pdb tests/unit/core/test_config.py::test_default_settings

# Show local variables on failure
pytest --tb=long tests/
```

## ğŸ”„ Continuous Improvement

### Test Maintenance

1. **Regular Updates**: Keep tests current with code changes
2. **Refactor Tests**: Improve test clarity and maintainability
3. **Remove Redundancy**: Eliminate duplicate test coverage
4. **Performance**: Optimize slow-running tests

### Coverage Improvement

1. **Identify Gaps**: Use coverage reports to find untested code
2. **Add Edge Cases**: Test error conditions and edge cases
3. **Integration Tests**: Add integration tests for complex workflows
4. **Performance Tests**: Add benchmarks for critical paths

## ğŸ“š Additional Resources

- [pytest Documentation](https://docs.pytest.org/)
- [Jest Documentation](https://jestjs.io/docs/)
- [React Testing Library](https://testing-library.com/docs/react-testing-library/)
- [MSW Documentation](https://mswjs.io/docs/)
- [Coverage.py Documentation](https://coverage.readthedocs.io/)

## ğŸ¤ Contributing

1. **Write Tests First**: TDD approach preferred
2. **Maintain Coverage**: Ensure new code has 95%+ coverage
3. **Follow Patterns**: Use established testing patterns
4. **Document Tests**: Add docstrings for complex test scenarios

---

**Test Suite Status**: âœ… 95% Coverage Achieved  
**Last Updated**: Generated automatically  
**Maintainer**: Legal AI Development Team